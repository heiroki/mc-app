# backend/inference_manager.pyï¼ˆæ–°è¦ãƒ»å®Œå…¨ç‰ˆï¼‰

import asyncio
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
import traceback
import logging
import psutil
from llama_cpp import Llama
from config import MODEL_PATH

logger = logging.getLogger(__name__)

def get_optimal_threads():
    """
    æœ€é©ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’è¨ˆç®—
    
    Returns:
        int: æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
    """
    try:
        physical_cores = psutil.cpu_count(logical=False)
        if physical_cores is None:
            physical_cores = 2  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # ç‰©ç†ã‚³ã‚¢ã®80%ã‚’ä½¿ç”¨ï¼ˆä»–ãƒ—ãƒ­ã‚»ã‚¹ã®ãŸã‚ä½™è£•ã‚’æŒãŸã›ã‚‹ï¼‰
        optimal_threads = max(1, int(physical_cores * 0.8))
        
        logger.info(f"ğŸ’» CPUæƒ…å ±: ç‰©ç†ã‚³ã‚¢={physical_cores}, æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰={optimal_threads}")
        return optimal_threads
    except Exception as e:
        logger.warning(f"âš ï¸ CPUæƒ…å ±å–å¾—å¤±æ•—: {e}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(2)ã‚’ä½¿ç”¨")
        return 2

def get_optimal_batch_size():
    """
    ãƒ¡ãƒ¢ãƒªã«å¿œã˜ãŸæœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º
    
    Returns:
        int: ãƒãƒƒãƒã‚µã‚¤ã‚º
    """
    try:
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        if available_memory_gb > 8:
            batch_size = 512
        elif available_memory_gb > 4:
            batch_size = 256
        else:
            batch_size = 128
        
        logger.info(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: åˆ©ç”¨å¯èƒ½={available_memory_gb:.1f}GB, ãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}")
        return batch_size
    except Exception as e:
        logger.warning(f"âš ï¸ ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—å¤±æ•—: {e}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(256)ã‚’ä½¿ç”¨")
        return 256

class InferenceManager:
    """æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®ç®¡ç†ãƒ»éš”é›¢ãƒ»å†åˆæœŸåŒ–"""
    
    def __init__(self, model_path: str, max_workers: int = 1):
        """
        InferenceManagerã‚’åˆæœŸåŒ–
        
        Args:
            model_path (str): ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            max_workers (int): ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
        """
        self.model_path = model_path
        self.llm = None
        self.lock = Lock()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.error_count = 0
        self.max_errors = 3  # 3å›é€£ç¶šã‚¨ãƒ©ãƒ¼ã§å†åˆæœŸåŒ–
        
        self._initialize()
    
    def _initialize(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        try:
            logger.info("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            logger.info(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {self.model_path}")
            
            if not self.model_path:
                raise ValueError("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=2048,                      # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º
                n_threads=get_optimal_threads(), # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆè‡ªå‹•æœ€é©åŒ–ï¼‰
                n_batch=get_optimal_batch_size(), # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆè‡ªå‹•æœ€é©åŒ–ï¼‰
                use_mlock=True,                  # ãƒ¡ãƒ¢ãƒªãƒ­ãƒƒã‚¯ï¼ˆã‚¹ãƒ¯ãƒƒãƒ—é˜²æ­¢ï¼‰
                verbose=False                     # è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶
            )
            
            self.error_count = 0
            logger.info("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _reinitialize(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆæœŸåŒ–"""
        logger.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆæœŸåŒ–ã—ã¾ã™...")
        
        try:
            # å¤ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç ´æ£„
            if self.llm is not None:
                del self.llm
                self.llm = None
            
            # å†åˆæœŸåŒ–
            self._initialize()
            logger.info("âœ… å†åˆæœŸåŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ å†åˆæœŸåŒ–å¤±æ•—: {e}")
            raise
    
    def _inference_worker(self, prompt: str, **kwargs):
        """
        æ¨è«–ã‚’å®Ÿè¡Œï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
        
        Args:
            prompt (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            **kwargs: æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Raises:
            RuntimeError: ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ
            Exception: æ¨è«–ã‚¨ãƒ©ãƒ¼
        """
        try:
            with self.lock:  # åŒæ™‚å®Ÿè¡Œã‚’é˜²ã
                if self.llm is None:
                    raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
                logger.debug(f"ğŸ¤– æ¨è«–é–‹å§‹: {prompt[:50]}...")
                
                output = self.llm(
                    prompt=prompt,
                    max_tokens=kwargs.get('max_tokens', 512),
                    temperature=kwargs.get('temperature', 0.7),
                    top_p=kwargs.get('top_p', 0.9),
                    stop=["</s>", "\n\n"],
                    echo=False
                )
                
                # æˆåŠŸã—ãŸã‚‰ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ
                self.error_count = 0
                
                result = output["choices"][0]["text"].strip()
                logger.debug(f"âœ… æ¨è«–å®Œäº†: {len(result)}æ–‡å­—")
                
                return result
        
        except Exception as e:
            self.error_count += 1
            logger.error(f"âš ï¸ æ¨è«–ã‚¨ãƒ©ãƒ¼ ({self.error_count}/{self.max_errors}): {e}")
            
            # é€£ç¶šã‚¨ãƒ©ãƒ¼ãŒé–¾å€¤ã‚’è¶…ãˆãŸã‚‰å†åˆæœŸåŒ–
            if self.error_count >= self.max_errors:
                logger.warning("ğŸ”„ ã‚¨ãƒ©ãƒ¼å›æ•°ãŒé–¾å€¤ã‚’è¶…ãˆã¾ã—ãŸã€‚å†åˆæœŸåŒ–ã—ã¾ã™ã€‚")
                try:
                    self._reinitialize()
                except Exception as reinit_error:
                    logger.error(f"âŒ å†åˆæœŸåŒ–ã‚‚å¤±æ•—ã—ã¾ã—ãŸ: {reinit_error}")
            
            raise
    
    async def generate(self, prompt: str, timeout: int = 60, **kwargs):
        """
        éåŒæœŸã§æ¨è«–ã‚’å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        
        Args:
            prompt (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            timeout (int): ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
            **kwargs: æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Raises:
            TimeoutError: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆ
            Exception: ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
        """
        loop = asyncio.get_event_loop()
        
        try:
            # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§æ¨è«–ã‚’å®Ÿè¡Œ
            result = await asyncio.wait_for(
                loop.run_in_executor(
                    self.executor,
                    self._inference_worker,
                    prompt,
                    kwargs
                ),
                timeout=timeout
            )
            return result
        
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout}ç§’)")
            raise TimeoutError(f"æ¨è«–ãŒ{timeout}ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        
        except Exception as e:
            logger.error(f"âŒ æ¨è«–å¤±æ•—: {e}")
            raise
    
    def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        logger.info("ğŸ›‘ æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­...")
        
        self.executor.shutdown(wait=True)
        
        if self.llm is not None:
            del self.llm
            self.llm = None
        
        logger.info("âœ… æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
inference_manager = None

def get_inference_manager():
    """
    InferenceManagerã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³å–å¾—
    
    Returns:
        InferenceManager: æ¨è«–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    
    Raises:
        Exception: åˆæœŸåŒ–å¤±æ•—æ™‚
    """
    global inference_manager
    
    if inference_manager is None:
        logger.info("ğŸ”§ æ¨è«–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–ä¸­...")
        
        if MODEL_PATH is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚config.pyã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        inference_manager = InferenceManager(model_path=MODEL_PATH)
    
    return inference_manager
